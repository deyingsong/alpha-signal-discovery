{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566faa4c",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Optimized Trade Execution\n",
    "**Authors**: Yuriy Nevmyvaka, Yi Feng, & Michael Kearns \\\n",
    "**Source**:  ICML \\\n",
    "**Year**: 2006 \\\n",
    "**Domain**: RL, Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e143c72",
   "metadata": {},
   "source": [
    "## 2. Mathematical Essence\n",
    "#### Goal\n",
    "to sell $V$ shares in time horizon $H$\n",
    "#### States\n",
    "$x_m=\\langle t,i, o_1, ..., o_R \\rangle \\in X$, a vector of state variables that describes the current configuration of the system. \\\n",
    "$t$, elapsed time \\\n",
    "$i$, remaining inventory \\\n",
    "$o_j$, market variables \n",
    "#### Rewards\n",
    "the proceeds (cash inflows or outflows, depending on whether we are selling or buying) from any (partial) execution of the limit order placed. \n",
    "#### Actions\n",
    "Action space is a simple limit order price at which to reposition all of our remaining inventory, relative to the current ask or bid, can be positive, zero, or negative.\n",
    "#### Assumptions\n",
    "1. Trade execution is (approximately) Markovian.\n",
    "2. Our own actions do not affect the behavior of other market participant, i.e., actions only affect private variables $t$ and $i$.\n",
    "#### Algorithm\n",
    "Q-learning: in every state we encounter, we try all possible actions and update the expected cost associated with taking each action and following the optimal strategy afterwards. \\\n",
    "Cost update rule:\n",
    "$$ c(x,a)=\\frac{n}{n+1}c(x,a) + \\frac{1}{n+1}[c_{im}(x,a)+\\argmax c(y,p)]$$\n",
    "where $c(x,a)$ is the cost of taking action $a$ in the state $x$ and following the optimal strategy in all subsequent states; $c_{im}(x,a)$ is the immediate (1-step) cost of taking $a$ in state $x$; $y$ is the new state where we end up after taking $a$ in $x$; $n$ is the number of times we have tried $a$ in $x$, and $p$ is the action taken in $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938fb6a",
   "metadata": {},
   "source": [
    "## 3. Implementation Notes\n",
    "\n",
    "#### Algorithm: Optimal_strategy($V, H, T, I, L$)\n",
    "**For** $t = T$ to $0$  \n",
    " **While** (not end of data):  \n",
    "  Transform (order book) $\\to$ $o_1, ..., o_R$ \\\n",
    "  **For** $i = 0$ to $I$ {  \n",
    "   **For** $a = 0$ to $L$ {  \n",
    "    Set $x = \\{t, i, o_1, ...,  o_R\\}$  \n",
    "    Simulate transition $x \\to y$\n",
    "    Calculate $c_im(x, a)$  \n",
    "    Look up $\\argmax c(y,p)$ \n",
    "    Update $c(\\langle t, i, o_1, ..., o_R\\rangle, a)$  \n",
    "   }  \n",
    "  }  \n",
    " **End While**  \n",
    " Select the highest-payout action $\\argmax c(y,p)$ in every state y to output optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb9165",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
