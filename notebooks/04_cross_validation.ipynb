{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Cross-Validation and Model Selection\n",
    "## Walk-Forward Analysis for Time Series\n",
    "\n",
    "This notebook implements proper time series cross-validation to avoid lookahead bias.\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Walk-Forward Validation**: Train on expanding windows\n",
    "2. **Purging**: Remove overlapping samples\n",
    "3. **Embargo**: Add gap between train and test\n",
    "4. **Combinatorial Purged Cross-Validation**: Advanced technique for financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "root_folder = './'  # Define the root folder for saving files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_all = pd.read_csv(f'{root_folder}data/processed/engineered_features.csv')\n",
    "y = pd.read_csv(f'{root_folder}data/processed/aligned_targets.csv').squeeze()\n",
    "\n",
    "# Load feature sets\n",
    "with open(f'{root_folder}data/processed/feature_sets.json', 'r') as f:\n",
    "    feature_sets = json.load(f)\n",
    "\n",
    "# Use best feature set from previous analysis\n",
    "feature_set = feature_sets['union']\n",
    "X_selected = X_all[feature_set]\n",
    "\n",
    "print(f\"Data shape: {X_selected.shape}\")\n",
    "print(f\"Using {len(feature_set)} selected features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Walk-Forward Cross-Validation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalkForwardCV:\n",
    "    \"\"\"Walk-forward cross-validation with purging and embargo.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, test_size=90, embargo=1, expanding=True):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.embargo = embargo\n",
    "        self.expanding = expanding\n",
    "    \n",
    "    def split(self, X, y=None):\n",
    "        \"\"\"Generate train/test splits.\"\"\"\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Calculate initial train size\n",
    "        min_train_size = max(100, n_samples // (self.n_splits + 1))\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            if self.expanding:\n",
    "                # Expanding window\n",
    "                train_start = 0\n",
    "                train_end = min_train_size + i * self.test_size\n",
    "            else:\n",
    "                # Rolling window\n",
    "                train_start = max(0, i * self.test_size)\n",
    "                train_end = min_train_size + i * self.test_size\n",
    "            \n",
    "            # Apply embargo\n",
    "            test_start = train_end + self.embargo\n",
    "            test_end = min(test_start + self.test_size, n_samples)\n",
    "            \n",
    "            if test_end > n_samples or test_start >= n_samples:\n",
    "                break\n",
    "            \n",
    "            train_idx = np.arange(train_start, train_end)\n",
    "            test_idx = np.arange(test_start, test_end)\n",
    "            \n",
    "            yield train_idx, test_idx\n",
    "    \n",
    "    def plot_splits(self, X):\n",
    "        \"\"\"Visualize the CV splits.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        for i, (train_idx, test_idx) in enumerate(self.split(X)):\n",
    "            # Plot train\n",
    "            ax.barh(i, len(train_idx), left=train_idx[0], height=0.3, \n",
    "                   color='blue', alpha=0.7, label='Train' if i == 0 else '')\n",
    "            # Plot test\n",
    "            ax.barh(i, len(test_idx), left=test_idx[0], height=0.3, \n",
    "                   color='red', alpha=0.7, label='Test' if i == 0 else '')\n",
    "            # Plot embargo\n",
    "            if self.embargo > 0:\n",
    "                embargo_start = train_idx[-1] + 1\n",
    "                ax.barh(i, self.embargo, left=embargo_start, height=0.3, \n",
    "                       color='gray', alpha=0.5, label='Embargo' if i == 0 else '')\n",
    "        \n",
    "        ax.set_xlabel('Sample Index')\n",
    "        ax.set_ylabel('CV Fold')\n",
    "        ax.set_title('Walk-Forward Cross-Validation Splits')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize CV splits\n",
    "cv = WalkForwardCV(n_splits=5, test_size=200, embargo=5, expanding=True)\n",
    "cv.plot_splits(X_selected)\n",
    "\n",
    "# Count actual splits\n",
    "n_actual_splits = sum(1 for _ in cv.split(X_selected))\n",
    "print(f\"\\nNumber of CV splits: {n_actual_splits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, X, y, cv, return_predictions=False):\n",
    "    \"\"\"Perform cross-validation and return metrics.\"\"\"\n",
    "\n",
    "    fold_metrics = []\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    all_indices = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(cv.split(X)):\n",
    "        # Split data\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "        X_test_scaled = scaler.transform(X_test_fold)\n",
    "\n",
    "        # Train model\n",
    "        if hasattr(model, 'fit'):\n",
    "            if isinstance(model, (xgb.XGBRegressor, lgb.LGBMRegressor)):\n",
    "                # For XGBoost and LightGBM, try to use early stopping if supported\n",
    "                try:\n",
    "                    model.fit(\n",
    "                        X_train_scaled, y_train_fold,\n",
    "                        eval_set=[(X_test_scaled, y_test_fold)],\n",
    "                        early_stopping_rounds=20,\n",
    "                        verbose=False,\n",
    "                    )\n",
    "                except TypeError:\n",
    "                    # Fallback: some versions/wrappers don't accept these kwargs\n",
    "                    try:\n",
    "                        model.fit(X_train_scaled, y_train_fold, eval_set=[(X_test_scaled, y_test_fold)])\n",
    "                    except TypeError:\n",
    "                        model.fit(X_train_scaled, y_train_fold)\n",
    "            else:\n",
    "                model.fit(X_train_scaled, y_train_fold)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test_fold, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        # Direction accuracy\n",
    "        direction_acc = np.mean(np.sign(y_test_fold) == np.sign(y_pred))\n",
    "\n",
    "        # Information coefficient\n",
    "        from scipy.stats import spearmanr\n",
    "        ic, _ = spearmanr(y_test_fold, y_pred)\n",
    "\n",
    "        # Store metrics\n",
    "        fold_metrics.append({\n",
    "            'fold': fold,\n",
    "            'train_size': len(train_idx),\n",
    "            'test_size': len(test_idx),\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'direction_acc': direction_acc,\n",
    "            'ic': ic\n",
    "        })\n",
    "\n",
    "        if return_predictions:\n",
    "            # Ensure predictions and actuals are flat lists/arrays\n",
    "            all_predictions.extend(np.asarray(y_pred).ravel().tolist())\n",
    "            all_actuals.extend(np.asarray(y_test_fold).ravel().tolist())\n",
    "            all_indices.extend(test_idx.tolist())\n",
    "\n",
    "    # Calculate average metrics\n",
    "    metrics_df = pd.DataFrame(fold_metrics)\n",
    "    avg_metrics = metrics_df[['mse', 'rmse', 'direction_acc', 'ic']].mean()\n",
    "    std_metrics = metrics_df[['mse', 'rmse', 'direction_acc', 'ic']].std()\n",
    "\n",
    "    results = {\n",
    "        'fold_metrics': metrics_df,\n",
    "        'avg_metrics': avg_metrics,\n",
    "        'std_metrics': std_metrics\n",
    "    }\n",
    "\n",
    "    if return_predictions:\n",
    "        results['predictions'] = np.array(all_predictions)\n",
    "        results['actuals'] = np.array(all_actuals)\n",
    "        results['indices'] = np.array(all_indices)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=10.0, random_state=42),\n",
    "    \n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=7,\n",
    "        min_samples_split=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'LightGBM': lgb.LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Run cross-validation for each model\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Running cross-validation for each model...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    results = cross_validate_model(\n",
    "        model, X_selected, y, cv, \n",
    "        return_predictions=True\n",
    "    )\n",
    "    \n",
    "    cv_results[model_name] = results\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nAverage Metrics:\")\n",
    "    for metric, value in results['avg_metrics'].items():\n",
    "        std = results['std_metrics'][metric]\n",
    "        print(f\"  {metric:15s}: {value:.6f} ± {std:.6f}\")\n",
    "    \n",
    "    # Print fold details\n",
    "    print(f\"\\nFold-by-fold results:\")\n",
    "    print(results['fold_metrics'][['fold', 'rmse', 'direction_acc', 'ic']].round(4))\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate CV results\n",
    "comparison_data = []\n",
    "for model_name, results in cv_results.items():\n",
    "    for _, row in results['fold_metrics'].iterrows():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Fold': row['fold'],\n",
    "            'RMSE': row['rmse'],\n",
    "            'Direction_Acc': row['direction_acc'],\n",
    "            'IC': row['ic']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=comparison_df, x='Model', y='RMSE', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('RMSE by Model (Lower is Better)')\n",
    "axes[0, 0].set_ylabel('RMSE')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Direction Accuracy comparison\n",
    "sns.boxplot(data=comparison_df, x='Model', y='Direction_Acc', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Direction Accuracy by Model (Higher is Better)')\n",
    "axes[0, 1].set_ylabel('Direction Accuracy')\n",
    "axes[0, 1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IC comparison\n",
    "sns.boxplot(data=comparison_df, x='Model', y='IC', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Information Coefficient by Model')\n",
    "axes[1, 0].set_ylabel('IC (Spearman Correlation)')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average metrics comparison\n",
    "avg_metrics_comparison = pd.DataFrame({\n",
    "    model: results['avg_metrics'] \n",
    "    for model, results in cv_results.items()\n",
    "}).T\n",
    "\n",
    "avg_metrics_comparison[['rmse', 'direction_acc', 'ic']].plot(\n",
    "    kind='bar', ax=axes[1, 1]\n",
    ")\n",
    "axes[1, 1].set_title('Average Metrics Comparison')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend(['RMSE', 'Direction Acc', 'IC'])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.suptitle('Cross-Validation Results Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction stability across folds\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, results) in enumerate(cv_results.items()):\n",
    "    if idx >= 4:\n",
    "        break\n",
    "    \n",
    "    predictions = results['predictions']\n",
    "    actuals = results['actuals']\n",
    "    \n",
    "    # Scatter plot of predictions vs actuals\n",
    "    axes[idx].scatter(actuals, predictions, alpha=0.5, s=1)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    min_val = min(actuals.min(), predictions.min())\n",
    "    max_val = max(actuals.max(), predictions.max())\n",
    "    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, alpha=0.5)\n",
    "    \n",
    "    # Add regression line\n",
    "    from scipy import stats\n",
    "    slope, intercept, r_value, _, _ = stats.linregress(actuals, predictions)\n",
    "    x_line = np.array([min_val, max_val])\n",
    "    y_line = slope * x_line + intercept\n",
    "    axes[idx].plot(x_line, y_line, 'b-', lw=2, alpha=0.7, \n",
    "                  label=f'y={slope:.3f}x+{intercept:.3f}\\nR²={r_value**2:.3f}')\n",
    "    \n",
    "    axes[idx].set_xlabel('Actual Returns')\n",
    "    axes[idx].set_ylabel('Predicted Returns')\n",
    "    axes[idx].set_title(f'{model_name}: Out-of-Sample Predictions')\n",
    "    axes[idx].legend(loc='upper left')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction Quality Across All CV Folds', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical tests to compare models\n",
    "from scipy import stats\n",
    "\n",
    "def compare_models_statistically(cv_results):\n",
    "    \"\"\"Perform pairwise statistical tests between models.\"\"\"\n",
    "\n",
    "    model_names = list(cv_results.keys())\n",
    "    n_models = len(model_names)\n",
    "\n",
    "    # Create comparison matrix for RMSE\n",
    "    p_values = np.ones((n_models, n_models))\n",
    "\n",
    "    for i in range(n_models):\n",
    "        for j in range(i+1, n_models):\n",
    "            model1 = model_names[i]\n",
    "            model2 = model_names[j]\n",
    "\n",
    "            # Get RMSE values for each fold\n",
    "            rmse1 = cv_results[model1]['fold_metrics']['rmse'].values\n",
    "            rmse2 = cv_results[model2]['fold_metrics']['rmse'].values\n",
    "\n",
    "            # Paired t-test (since same folds)\n",
    "            t_stat, p_value = stats.ttest_rel(rmse1, rmse2)\n",
    "\n",
    "            p_values[i, j] = p_value\n",
    "            p_values[j, i] = p_value\n",
    "\n",
    "    # Create DataFrame\n",
    "    p_values_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
    "\n",
    "    return p_values_df\n",
    "\n",
    "# Perform comparison\n",
    "p_values_df = compare_models_statistically(cv_results)\n",
    "\n",
    "# Visualize p-values\n",
    "plt.figure(figsize=(8, 6))\n",
    "mask = np.triu(np.ones_like(p_values_df, dtype=bool))\n",
    "sns.heatmap(p_values_df, mask=mask, annot=True, fmt='.3f', \n",
    "            cmap='RdYlGn_r', center=0.05, vmin=0, vmax=0.2,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Pairwise Statistical Comparison (p-values)\\nPaired t-test on RMSE')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Statistical Significance (p < 0.05):\")\n",
    "print(\"=\"*50)\n",
    "# Ensure model_names is defined for this reporting loop\n",
    "model_names = list(cv_results.keys())\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i+1, len(model_names)):\n",
    "        p_val = p_values_df.iloc[i, j]\n",
    "        if p_val < 0.05:\n",
    "            model1 = model_names[i]\n",
    "            model2 = model_names[j]\n",
    "            avg_rmse1 = cv_results[model1]['avg_metrics']['rmse']\n",
    "            avg_rmse2 = cv_results[model2]['avg_metrics']['rmse']\n",
    "            better = model1 if avg_rmse1 < avg_rmse2 else model2\n",
    "            print(f\"{model1} vs {model2}: p={p_val:.4f} - {better} is significantly better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank models by multiple criteria\n",
    "ranking_data = []\n",
    "\n",
    "for model_name, results in cv_results.items():\n",
    "    avg_metrics = results['avg_metrics']\n",
    "    std_metrics = results['std_metrics']\n",
    "    \n",
    "    # Calculate composite score (lower is better)\n",
    "    # Weighted combination of normalized metrics\n",
    "    composite_score = (\n",
    "        avg_metrics['rmse'] * 1.0 +  # Lower is better\n",
    "        (1 - avg_metrics['direction_acc']) * 0.5 +  # Higher is better, so invert\n",
    "        (1 - avg_metrics['ic']) * 0.3 +  # Higher is better, so invert\n",
    "        std_metrics['rmse'] * 0.2  # Penalize high variance\n",
    "    )\n",
    "    \n",
    "    ranking_data.append({\n",
    "        'Model': model_name,\n",
    "        'Avg_RMSE': avg_metrics['rmse'],\n",
    "        'Std_RMSE': std_metrics['rmse'],\n",
    "        'Avg_Direction_Acc': avg_metrics['direction_acc'],\n",
    "        'Avg_IC': avg_metrics['ic'],\n",
    "        'Composite_Score': composite_score\n",
    "    })\n",
    "\n",
    "ranking_df = pd.DataFrame(ranking_data)\n",
    "ranking_df = ranking_df.sort_values('Composite_Score')\n",
    "ranking_df['Rank'] = range(1, len(ranking_df) + 1)\n",
    "\n",
    "print(\"\\nModel Ranking (Cross-Validation Performance)\")\n",
    "print(\"=\"*80)\n",
    "print(ranking_df.set_index('Rank').round(4))\n",
    "\n",
    "best_model = ranking_df.iloc[0]['Model']\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_model}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Average RMSE: {ranking_df.iloc[0]['Avg_RMSE']:.6f}\")\n",
    "print(f\"Direction Accuracy: {ranking_df.iloc[0]['Avg_Direction_Acc']:.4f}\")\n",
    "print(f\"Information Coefficient: {ranking_df.iloc[0]['Avg_IC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CV results\n",
    "import pickle\n",
    "\n",
    "# Save full CV results\n",
    "with open(f'{root_folder}data/results/cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results, f)\n",
    "print(\"✓ Saved full CV results\")\n",
    "\n",
    "# Save ranking\n",
    "ranking_df.to_csv(f'{root_folder}data/results/model_ranking_cv.csv', index=False)\n",
    "print(\"✓ Saved model ranking\")\n",
    "\n",
    "# Save best model selection\n",
    "best_model_info = {\n",
    "    'model_name': best_model,\n",
    "    'avg_metrics': cv_results[best_model]['avg_metrics'].to_dict(),\n",
    "    'std_metrics': cv_results[best_model]['std_metrics'].to_dict(),\n",
    "    'feature_set': feature_set,\n",
    "    'cv_params': {\n",
    "        'n_splits': cv.n_splits,\n",
    "        'test_size': cv.test_size,\n",
    "        'embargo': cv.embargo,\n",
    "        'expanding': cv.expanding\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{root_folder}data/results/best_model_cv.json', 'w') as f:\n",
    "    json.dump(best_model_info, f, indent=2)\n",
    "print(\"✓ Saved best model information\")\n",
    "\n",
    "print(\"\\nCross-validation complete!\")\n",
    "print(f\"Best model ({best_model}) selected for backtesting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (quant_trade)",
   "language": "python",
   "name": "quant_trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
