{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Exploratory Data Analysis\n",
    "## Quantitative Trading Strategy Development\n",
    "\n",
    "### Objectives:\n",
    "1. Load and understand the data structure\n",
    "2. Analyze statistical properties of returns and signals\n",
    "3. Identify patterns, anomalies, and relationships\n",
    "4. Visualize key insights\n",
    "5. Form hypotheses for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(filepath='../data/raw/synthetic_data.csv', Tcost=0.005):\n",
    "    \"\"\"Load and clean the data from CSV file.\"\"\"\n",
    "    # Read CSV file\n",
    "    df_raw = pd.read_csv(filepath)\n",
    "    \n",
    "    # Extract transaction cost\n",
    "    transaction_cost = Tcost\n",
    "    print(f\"Transaction cost: {transaction_cost:.3f} ({transaction_cost*100:.1f}%)\")\n",
    "    \n",
    "    # Find header row\n",
    "    # Robustly locate header row (look for a cell that equals 'Date' ignoring case/whitespace or contains 'date')\n",
    "    if df_raw.columns.astype(str).str.strip().str.lower().str.contains('date').any():\n",
    "        header = df_raw.columns.astype(str).str.strip()\n",
    "        df = df_raw.copy().reset_index(drop=True)\n",
    "        print(\"Header inferred from CSV column names.\")\n",
    "        \n",
    "\n",
    "    # Normalize column names and assign\n",
    "    header = header.astype(str).str.strip().str.replace(r'\\s+', '_', regex=True)\n",
    "    df.columns = header\n",
    "    # Standardize column name casing to be robust but keep capital 'Date' for downstream cells\n",
    "    df.columns = [col if col.lower() != 'date' else 'Date' for col in df.columns]\n",
    "\n",
    "    # Drop entirely empty columns\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # Parse Date column\n",
    "    df['Date'] = pd.to_numeric(df['Date'], errors='coerce')\n",
    "    \n",
    "    # Convert other columns to numeric where appropriate\n",
    "    for col in df.columns:\n",
    "        if col == 'Date':\n",
    "            continue\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df, transaction_cost\n",
    "\n",
    "# Load the data\n",
    "df, transaction_cost = load_data()\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"Data Quality Report\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total observations: {len(df)}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Analysis of Asset Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze return distribution\n",
    "returns = df['Returns'].values\n",
    "\n",
    "# Calculate statistics\n",
    "return_stats = {\n",
    "    'Mean': np.mean(returns),\n",
    "    'Median': np.median(returns),\n",
    "    'Std Dev': np.std(returns),\n",
    "    'Skewness': stats.skew(returns),\n",
    "    'Kurtosis': stats.kurtosis(returns, fisher=False),\n",
    "    'Min': np.min(returns),\n",
    "    'Max': np.max(returns),\n",
    "    '5% Percentile': np.percentile(returns, 5),\n",
    "    '95% Percentile': np.percentile(returns, 95),\n",
    "    'Annualized Return': np.mean(returns) * 252,\n",
    "    'Annualized Volatility': np.std(returns) * np.sqrt(252),\n",
    "    'Sharpe Ratio': (np.mean(returns) * 252) / (np.std(returns) * np.sqrt(252))\n",
    "}\n",
    "\n",
    "print(\"Asset Returns Statistics\")\n",
    "print(\"=\"*40)\n",
    "for key, value in return_stats.items():\n",
    "    print(f\"{key:20s}: {value:12.6f}\")\n",
    "\n",
    "# Normality test\n",
    "jb_stat, jb_pvalue, _, _ = jarque_bera(returns)\n",
    "print(f\"\\nJarque-Bera test: statistic={jb_stat:.4f}, p-value={jb_pvalue:.4f}\")\n",
    "if jb_pvalue < 0.05:\n",
    "    print(\"Returns are NOT normally distributed (reject null hypothesis)\")\n",
    "else:\n",
    "    print(\"Returns appear to be normally distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize return distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Time series plot\n",
    "axes[0, 0].plot(df['Date'], returns, linewidth=0.5, alpha=0.8)\n",
    "axes[0, 0].set_title('Asset Returns Over Time')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Returns')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with normal overlay\n",
    "axes[0, 1].hist(returns, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "x = np.linspace(returns.min(), returns.max(), 100)\n",
    "axes[0, 1].plot(x, stats.norm.pdf(x, np.mean(returns), np.std(returns)), \n",
    "                'r-', linewidth=2, label='Normal')\n",
    "axes[0, 1].set_title('Return Distribution')\n",
    "axes[0, 1].set_xlabel('Returns')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(returns, dist=\"norm\", plot=axes[0, 2])\n",
    "axes[0, 2].set_title('Q-Q Plot')\n",
    "\n",
    "# Cumulative returns\n",
    "cumulative_returns = np.cumprod(1 + returns) - 1\n",
    "axes[1, 0].plot(df['Date'], cumulative_returns, linewidth=1.5)\n",
    "axes[1, 0].set_title('Cumulative Returns')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Cumulative Return')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling volatility (21-day)\n",
    "rolling_vol = pd.Series(returns).rolling(window=21).std() * np.sqrt(252)\n",
    "axes[1, 1].plot(df['Date'], rolling_vol, linewidth=1)\n",
    "axes[1, 1].set_title('Rolling 21-Day Annualized Volatility')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Volatility')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by quintiles\n",
    "date_quintiles = pd.qcut(df['Date'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "df['Quintile'] = date_quintiles\n",
    "df.boxplot(column='Returns', by='Quintile', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Returns by Time Period')\n",
    "axes[1, 2].set_xlabel('Time Quintile')\n",
    "axes[1, 2].set_ylabel('Returns')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis of Signal Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze signal statistics\n",
    "signal_cols = df.columns.drop(['Date', 'Returns', 'Quintile']).tolist()\n",
    "\n",
    "signal_stats = pd.DataFrame({\n",
    "    'Mean': df[signal_cols].mean(),\n",
    "    'Std': df[signal_cols].std(),\n",
    "    'Skewness': df[signal_cols].skew(),\n",
    "    'Kurtosis': df[signal_cols].kurtosis() + 3,  # Convert to standard kurtosis\n",
    "    'Min': df[signal_cols].min(),\n",
    "    'Max': df[signal_cols].max(),\n",
    "    '25%': df[signal_cols].quantile(0.25),\n",
    "    '75%': df[signal_cols].quantile(0.75)\n",
    "})\n",
    "\n",
    "print(\"Signal Statistics Summary\")\n",
    "print(\"=\"*80)\n",
    "print(signal_stats.round(6))\n",
    "\n",
    "# Check if signals have identical properties\n",
    "unique_stats = signal_stats.nunique()\n",
    "if (unique_stats <= 1).any():\n",
    "    print(\"\\n WARNING: Some statistics are identical across signals!\")\n",
    "    print(\"Identical statistics:\")\n",
    "    print(unique_stats[unique_stats <= 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize signal distributions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(signal_cols):\n",
    "    axes[i].hist(df[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'Signal {col} Distribution')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics text\n",
    "    stats_text = f'μ={df[col].mean():.3f}\\nσ={df[col].std():.3f}'\n",
    "    axes[i].text(0.7, 0.9, stats_text, transform=axes[i].transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = df[signal_cols].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.3f', \n",
    "            cmap='coolwarm', center=0, vmin=-1, vmax=1,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Signal Correlation Matrix', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Check for multicollinearity\n",
    "high_corr_pairs = []\n",
    "for i in range(len(signal_cols)):\n",
    "    for j in range(i+1, len(signal_cols)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((signal_cols[i], signal_cols[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"High correlation pairs (|r| > 0.8):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "else:\n",
    "    print(\"No high correlations found (all |r| ≤ 0.8)\")\n",
    "    print(\"\\n✓ Signals appear to be orthogonal/uncorrelated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal-Return correlations\n",
    "signal_return_corr = pd.Series({\n",
    "    col: df[col].corr(df['Returns']) for col in signal_cols\n",
    "})\n",
    "\n",
    "# Lagged correlations\n",
    "lag_correlations = pd.DataFrame()\n",
    "for lag in range(1, 6):\n",
    "    lag_corr = pd.Series({\n",
    "        col: df[col].shift(lag).corr(df['Returns']) for col in signal_cols\n",
    "    })\n",
    "    lag_correlations[f'Lag_{lag}'] = lag_corr\n",
    "\n",
    "# Visualize signal-return correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Contemporaneous correlations\n",
    "axes[0].bar(signal_cols, signal_return_corr.values, color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('Signal-Return Correlations (Contemporaneous)')\n",
    "axes[0].set_xlabel('Signal')\n",
    "axes[0].set_ylabel('Correlation')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Lagged correlations heatmap\n",
    "sns.heatmap(lag_correlations.T, annot=True, fmt='.3f', cmap='RdBu_r', \n",
    "            center=0, vmin=-0.1, vmax=0.1, ax=axes[1], cbar_kws={\"shrink\": 0.8})\n",
    "axes[1].set_title('Lagged Signal-Return Correlations')\n",
    "axes[1].set_xlabel('Signal')\n",
    "axes[1].set_ylabel('Lag')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Signal-Return Correlations:\")\n",
    "print(signal_return_corr.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Series Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for stationarity\n",
    "def test_stationarity(series, name):\n",
    "    \"\"\"Perform Augmented Dickey-Fuller test for stationarity.\"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'\\nADF Test for {name}:')\n",
    "    print(f'ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"✓ {name} is stationary (reject H0)\")\n",
    "    else:\n",
    "        print(f\"✗ {name} is non-stationary (fail to reject H0)\")\n",
    "    return result[1] <= 0.05\n",
    "\n",
    "# Test returns and signals\n",
    "stationary_results = {}\n",
    "stationary_results['Returns'] = test_stationarity(df['Returns'], 'Returns')\n",
    "\n",
    "for col in signal_cols[:3]:  # Test first 3 signals as example\n",
    "    stationary_results[col] = test_stationarity(df[col], col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ACF for returns\n",
    "acf_returns = acf(df['Returns'].dropna(), nlags=40)\n",
    "axes[0, 0].bar(range(len(acf_returns)), acf_returns, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0, 0].axhline(y=1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].axhline(y=-1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_title('ACF - Returns')\n",
    "axes[0, 0].set_xlabel('Lag')\n",
    "axes[0, 0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# PACF for returns\n",
    "pacf_returns = pacf(df['Returns'].dropna(), nlags=40)\n",
    "axes[0, 1].bar(range(len(pacf_returns)), pacf_returns, color='coral', alpha=0.7)\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0, 1].axhline(y=1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].axhline(y=-1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_title('PACF - Returns')\n",
    "axes[0, 1].set_xlabel('Lag')\n",
    "axes[0, 1].set_ylabel('Partial Autocorrelation')\n",
    "\n",
    "# ACF for squared returns (volatility clustering)\n",
    "squared_returns = df['Returns'] ** 2\n",
    "acf_squared = acf(squared_returns.dropna(), nlags=40)\n",
    "axes[1, 0].bar(range(len(acf_squared)), acf_squared, color='green', alpha=0.7)\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1, 0].axhline(y=1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].axhline(y=-1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_title('ACF - Squared Returns (Volatility Clustering)')\n",
    "axes[1, 0].set_xlabel('Lag')\n",
    "axes[1, 0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# ACF for absolute returns\n",
    "abs_returns = np.abs(df['Returns'])\n",
    "acf_abs = acf(abs_returns.dropna(), nlags=40)\n",
    "axes[1, 1].bar(range(len(acf_abs)), acf_abs, color='purple', alpha=0.7)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1, 1].axhline(y=1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].axhline(y=-1.96/np.sqrt(len(df)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('ACF - Absolute Returns')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].set_ylabel('Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings and Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "findings = [\n",
    "    (\"Data Structure\", [\n",
    "        f\"Total observations: {len(df)}\",\n",
    "        f\"Features: 10 signals (F1-F10)\",\n",
    "        f\"Target: Asset returns\",\n",
    "        f\"Transaction cost: {transaction_cost:.1%}\"\n",
    "    ]),\n",
    "    \n",
    "    (\"Return Characteristics\", [\n",
    "        f\"Annualized return: {return_stats['Annualized Return']:.2%}\",\n",
    "        f\"Annualized volatility: {return_stats['Annualized Volatility']:.2%}\",\n",
    "        f\"Sharpe ratio: {return_stats['Sharpe Ratio']:.3f}\",\n",
    "        f\"Skewness: {return_stats['Skewness']:.3f} (slightly negative)\",\n",
    "        f\"Kurtosis: {return_stats['Kurtosis']:.3f} ({'leptokurtic' if return_stats['Kurtosis'] > 3 else 'platykurtic'})\",\n",
    "        f\"Normality: {'Rejected' if jb_pvalue < 0.05 else 'Not rejected'} (p={jb_pvalue:.4f})\"\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# Print formatted findings\n",
    "for section, bullets in findings:\n",
    "    print(\"\\n\" + section)\n",
    "    print(\"-\" * len(section))\n",
    "    for line in bullets:\n",
    "        print(f\"• {line}\")\n",
    "\n",
    "# Add a short signal-return correlation summary\n",
    "if 'signal_return_corr' in globals():\n",
    "    print(\"\\nSignal-Return Correlations (summary)\")\n",
    "    print(\"-\" * 36)\n",
    "    sorted_corr = signal_return_corr.sort_values(ascending=False)\n",
    "    print(\"Top 3 positive correlations:\")\n",
    "    for name, val in sorted_corr.head(3).items():\n",
    "        print(f\"• {name}: {val:.3f}\")\n",
    "    print(\"Top 3 negative correlations:\")\n",
    "    for name, val in sorted_corr.tail(3).items():\n",
    "        print(f\"• {name}: {val:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for next notebooks\n",
    "df.to_csv('../data/processed/clean_data.csv', index=False)\n",
    "print(\"Data saved to 'data/processed/clean_data.csv'\")\n",
    "\n",
    "# Save key statistics\n",
    "stats_summary = {\n",
    "    'transaction_cost': transaction_cost,\n",
    "    'return_stats': return_stats,\n",
    "    'signal_stats': signal_stats.to_dict(),\n",
    "    'signal_return_correlations': signal_return_corr.to_dict()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/processed/eda_summary1.json', 'w') as f:\n",
    "    json.dump(stats_summary, f, indent=2, default=str)\n",
    "print(\"Statistics saved to 'data/processed/eda_summary1.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
