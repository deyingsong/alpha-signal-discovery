"""
The ONLY deep learning approaches that have a chance on 1,500 samples
These are extremely regularized, tiny models designed for small datasets
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# ========================================
# OPTION 1: Tiny 1D CNN (Best DL Option)
# ========================================

class TinyConv1D(nn.Module):
    """
    1D CNN with <1000 parameters - often beats RNNs on small data
    Uses extreme regularization
    """
    def __init__(self, n_features, dropout=0.7):
        super().__init__()
        # Ultra-small architecture
        self.conv1 = nn.Conv1d(n_features, 8, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(8)
        self.dropout1 = nn.Dropout(dropout)
        
        self.conv2 = nn.Conv1d(8, 4, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(4)
        self.dropout2 = nn.Dropout(dropout)
        
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(4, 1)
        
        # Initialize with very small weights
        for m in self.modules():
            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # x: (batch, sequence_len, features)
        x = x.transpose(1, 2)  # (batch, features, sequence_len)
        
        x = self.conv1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.dropout1(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.dropout2(x)
        
        x = self.global_pool(x).squeeze(-1)
        x = self.fc(x)
        return x.squeeze()

# ========================================
# OPTION 2: Ultra-Tiny GRU 
# ========================================

class UltraTinyGRU(nn.Module):
    """
    GRU with extreme constraints for tiny datasets
    Total parameters: ~500-1000
    """
    def __init__(self, n_features, hidden_size=4, dropout=0.7):
        super().__init__()
        self.gru = nn.GRU(
            n_features, 
            hidden_size, 
            batch_first=True,
            bidirectional=False  # Keep it simple
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
        # Aggressive weight initialization
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param, gain=0.01)
    
    def forward(self, x):
        # Use only last hidden state
        _, h_n = self.gru(x)
        h_n = h_n.squeeze(0)
        h_n = self.dropout(h_n)
        out = self.fc(h_n)
        return out.squeeze()

# ========================================
# OPTION 3: Ensemble of Tiny Models
# ========================================

class TinyEnsemble(nn.Module):
    """
    Ensemble of multiple tiny models - helps reduce overfitting
    """
    def __init__(self, n_features, n_models=3):
        super().__init__()
        self.models = nn.ModuleList([
            UltraTinyGRU(n_features, hidden_size=3, dropout=0.6),
            UltraTinyGRU(n_features, hidden_size=4, dropout=0.7),
            TinyConv1D(n_features, dropout=0.6)
        ])
    
    def forward(self, x):
        outputs = [model(x) for model in self.models]
        return torch.stack(outputs).mean(dim=0)

# ========================================
# PRACTICAL IMPLEMENTATION
# ========================================

class SmallDataDeepTrader:
    """
    Complete implementation optimized for small financial datasets
    """
    
    def __init__(self, 
                 model_type='conv1d',  # 'conv1d', 'gru', or 'ensemble'
                 sequence_length=5,    # Keep short for small data
                 batch_size=32,
                 learning_rate=0.001,
                 weight_decay=0.1,     # Heavy L2 regularization
                 epochs=50,
                 patience=10):
        
        self.model_type = model_type
        self.sequence_length = sequence_length
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.epochs = epochs
        self.patience = patience
        
        self.model = None
        self.scaler = StandardScaler()
        self.best_model_state = None
        
    def create_sequences(self, X, y=None):
        """Convert to sequences for temporal models"""
        n_samples = len(X) - self.sequence_length + 1
        n_features = X.shape[1]
        
        sequences = np.zeros((n_samples, self.sequence_length, n_features))
        for i in range(n_samples):
            sequences[i] = X[i:i + self.sequence_length]
        
        if y is not None:
            # Align targets with sequences
            targets = y[self.sequence_length - 1:]
            return sequences, targets
        
        return sequences
    
    def add_noise_augmentation(self, X, noise_level=0.01):
        """Add Gaussian noise for data augmentation"""
        noise = np.random.normal(0, noise_level, X.shape)
        return X + noise
    
    def fit(self, X_train, y_train, X_val=None, y_val=None):
        """
        Train the model with heavy regularization and early stopping
        """
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # Create sequences
        X_seq, y_seq = self.create_sequences(X_train_scaled, y_train)
        
        # Validation data
        if X_val is not None and y_val is not None:
            X_val_scaled = self.scaler.transform(X_val)
            X_val_seq, y_val_seq = self.create_sequences(X_val_scaled, y_val)
        else:
            # Use last 20% as validation
            val_size = int(0.2 * len(X_seq))
            X_val_seq = X_seq[-val_size:]
            y_val_seq = y_seq[-val_size:]
            X_seq = X_seq[:-val_size]
            y_seq = y_seq[:-val_size]
        
        # Convert to tensors
        X_train_tensor = torch.FloatTensor(X_seq)
        y_train_tensor = torch.FloatTensor(y_seq)
        X_val_tensor = torch.FloatTensor(X_val_seq)
        y_val_tensor = torch.FloatTensor(y_val_seq)
        
        # Create model
        n_features = X_train.shape[1]
        if self.model_type == 'conv1d':
            self.model = TinyConv1D(n_features)
        elif self.model_type == 'gru':
            self.model = UltraTinyGRU(n_features)
        elif self.model_type == 'ensemble':
            self.model = TinyEnsemble(n_features)
        
        print(f"Model: {self.model_type}")
        print(f"Parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"Training samples: {len(X_train_tensor)}")
        
        # Optimizer with heavy weight decay
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        
        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=False
        )
        
        criterion = nn.MSELoss()
        
        # Training loop with early stopping
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []
        
        for epoch in range(self.epochs):
            # Training
            self.model.train()
            
            # Data augmentation - add noise each epoch
            X_augmented = self.add_noise_augmentation(X_seq, noise_level=0.005)
            X_augmented_tensor = torch.FloatTensor(X_augmented)
            
            optimizer.zero_grad()
            train_pred = self.model(X_augmented_tensor)
            train_loss = criterion(train_pred, y_train_tensor)
            
            # Add L1 regularization manually
            l1_lambda = 0.01
            l1_norm = sum(p.abs().sum() for p in self.model.parameters())
            train_loss = train_loss + l1_lambda * l1_norm
            
            train_loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
            
            optimizer.step()
            
            # Validation
            self.model.eval()
            with torch.no_grad():
                val_pred = self.model(X_val_tensor)
                val_loss = criterion(val_pred, y_val_tensor)
            
            train_losses.append(train_loss.item())
            val_losses.append(val_loss.item())
            
            # Learning rate scheduling
            scheduler.step(val_loss)
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self.best_model_state = self.model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1
                
            if patience_counter >= self.patience:
                print(f"Early stopping at epoch {epoch}")
                break
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")
        
        # Load best model
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
        
        return self
    
    def predict(self, X):
        """Make predictions"""
        X_scaled = self.scaler.transform(X)
        X_seq = self.create_sequences(X_scaled)
        
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_seq)
            predictions = self.model(X_tensor).numpy()
        
        # Pad predictions to match original length
        padded_predictions = np.zeros(len(X))
        padded_predictions[self.sequence_length - 1:] = predictions
        
        return padded_predictions

# ========================================
# COMPARISON WITH CLASSICAL METHODS
# ========================================

def compare_deep_vs_classical(n_samples=1500, n_features=115):
    """
    Direct comparison showing why classical ML wins on small data
    """
    print("="*70)
    print("DEEP LEARNING vs CLASSICAL ML on 1,500 SAMPLES")
    print("="*70)
    
    # Generate synthetic financial data
    np.random.seed(42)
    X = np.random.randn(n_samples, n_features)
    
    # Add some temporal structure
    for i in range(1, n_samples):
        X[i] = 0.95 * X[i-1] + 0.05 * np.random.randn(n_features)
    
    # Generate returns with some signal
    y = 0.01 * X[:, :5].mean(axis=1) + 0.001 * np.random.randn(n_samples)
    
    # Train/test split
    train_size = int(0.7 * n_samples)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    results = {}
    
    # 1. Ridge Regression
    print("\n1. Ridge Regression")
    from sklearn.linear_model import Ridge
    import time
    
    start = time.time()
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_train, y_train)
    ridge_time = time.time() - start
    ridge_pred = ridge.predict(X_test)
    ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))
    
    print(f"   Training time: {ridge_time:.3f}s")
    print(f"   Test RMSE: {ridge_rmse:.6f}")
    results['Ridge'] = {'time': ridge_time, 'rmse': ridge_rmse}
    
    # 2. XGBoost
    print("\n2. XGBoost")
    from xgboost import XGBRegressor
    
    start = time.time()
    xgb = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, verbosity=0)
    xgb.fit(X_train, y_train)
    xgb_time = time.time() - start
    xgb_pred = xgb.predict(X_test)
    xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
    
    print(f"   Training time: {xgb_time:.3f}s")
    print(f"   Test RMSE: {xgb_rmse:.6f}")
    results['XGBoost'] = {'time': xgb_time, 'rmse': xgb_rmse}
    
    # 3. Tiny Deep Learning
    print("\n3. Tiny Conv1D (Deep Learning)")
    
    start = time.time()
    deep_model = SmallDataDeepTrader(
        model_type='conv1d',
        sequence_length=5,
        epochs=30,
        patience=10
    )
    deep_model.fit(X_train, y_train)
    deep_time = time.time() - start
    deep_pred = deep_model.predict(X_test)
    deep_rmse = np.sqrt(mean_squared_error(y_test[4:], deep_pred[4:]))  # Account for sequence
    
    print(f"   Training time: {deep_time:.3f}s")
    print(f"   Test RMSE: {deep_rmse:.6f}")
    results['TinyConv1D'] = {'time': deep_time, 'rmse': deep_rmse}
    
    # Summary
    print("\n" + "="*70)
    print("RESULTS SUMMARY")
    print("="*70)
    
    # Sort by RMSE
    sorted_results = sorted(results.items(), key=lambda x: x[1]['rmse'])
    
    print("\nRanking by Test RMSE (lower is better):")
    for i, (model, metrics) in enumerate(sorted_results, 1):
        print(f"{i}. {model:15s}: RMSE = {metrics['rmse']:.6f}, Time = {metrics['time']:.2f}s")
    
    print("\nüìä Conclusion:")
    winner = sorted_results[0][0]
    print(f"   Winner: {winner}")
    
    if winner in ['Ridge', 'XGBoost']:
        print("   ‚úÖ Classical ML wins (as expected for small data)")
    else:
        print("   ‚ö†Ô∏è  Deep learning won (surprising! Probably lucky random seed)")
    
    print("""
    üí° Key Insights:
       ‚Ä¢ Deep learning takes 10-100x longer to train
       ‚Ä¢ Deep learning usually has worse RMSE on small data
       ‚Ä¢ Even tiny deep models struggle with 1,500 samples
       ‚Ä¢ Classical ML is the right choice for your dataset
    """)
    
    return results


if __name__ == "__main__":
    print("\nüöÄ Testing Deep Learning on Small Financial Data\n")
    
    # Run comparison
    results = compare_deep_vs_classical(n_samples=1500, n_features=115)
    
    print("\n" + "="*70)
    print("‚úÖ ANALYSIS COMPLETE")
    print("="*70)
    print("""
    Final Recommendations:
    1. DONT use standard LSTM/Transformer - they WILL overfit
    2. IF you must use deep learning, use TinyConv1D with dropout=0.7
    3. BETTER: Stick with XGBoost for your 1,500 samples
    4. BEST: Ensemble of Ridge + XGBoost
    """)