ridge:
  alpha_range: [0.1, 1.0, 10.0, 100.0]  # Simplified range
  cv_folds: 3  # Fewer folds for small data
  fit_intercept: true
  max_iter: 1000

xgboost:
  # Optimized for ~2000 samples
  n_estimators_range: [50, 100, 150]  # Fewer trees
  max_depth_range: [2, 3, 4]  # Shallow trees to prevent overfitting
  learning_rate_range: [0.05, 0.1, 0.15]  # Higher learning rates OK for small data
  reg_alpha_range: [0.1, 1.0]  # More regularization
  reg_lambda_range: [1.0, 2.0]  # More regularization
  subsample_range: [0.8]  # Fixed, good value
  colsample_bytree_range: [0.8]  # Fixed, good value
  min_child_weight_range: [5, 10]  # Higher values to prevent overfitting
  random_state: 42
  n_jobs: -1
  early_stopping_rounds: 10  # Fewer rounds for small data
  eval_metric: 'rmse'

random_forest:
  # Optimized for ~2000 samples
  n_estimators_range: [50, 100]  # Fewer trees
  max_depth_range: [3, 5, 7]  # Shallow to medium depth
  min_samples_split_range: [20, 50]  # High values to prevent overfitting
  min_samples_leaf_range: [10, 20]  # High values to prevent overfitting
  max_features_range: ['sqrt', 0.5]  # Limit features per split
  random_state: 42
  n_jobs: -1
  bootstrap: true
  oob_score: false  # Can enable for additional validation

lightgbm:
  # ONLY USE IF YOU HAVE >10,000 SAMPLES
  fast_mode: true  # Skip grid search, use good defaults
  early_stopping: false  # Not enough data for validation split
  

  n_estimators_range: [50, 100]  # Very few trees
  max_depth_range: [3, 4]  # Very shallow
  num_leaves_range: [7, 15]  # Very few leaves (must be < 2^max_depth)
  learning_rate_range: [0.1]  # Single higher value
  min_child_samples_range: [30]  # High value
  subsample_range: [0.8]  # Fixed
  colsample_bytree_range: [0.8]  # Fixed
  reg_alpha_range: [0.1]  # Fixed regularization
  reg_lambda_range: [0.1]  # Fixed regularization
  random_state: 42
  cv_folds: 2  # Minimal CV
  force_row_wise: true  # Better for small datasets
  force_col_wise: false  # Better for small datasets
  
# Recommended ensemble approach for your data size:
ensemble:
  # For 2000 samples, simple averaging often beats complex stacking
  method: 'weighted_average'
  base_models: ['ridge', 'xgboost']  # Just use the two best
  weights: [0.3, 0.7]  # More weight on XGBoost
  

feature_engineering:
  # Reduce complexity to avoid overfitting
  lag_periods: [1, 2, 3]  # Fewer lags
  polynomial_degree: 2  # Keep it simple
  interaction_features: false  # Can cause overfitting on small data
  rolling_windows: [5, 10]  # Fewer windows
  technical_indicators: true
  pca_components: 30  # Dimensionality reduction might help
  feature_selection_method: 'mutual_info'
  feature_selection_k: 30  # Select top 30 features only

# MLP (Multi-Layer Perceptron) - Optimized for 2000 samples
mlp:
  fast_mode: true  # Skip extensive grid search
  
  hidden_layer_sizes: 
    - [8]           # Ultra-tiny: single layer, 8 neurons (safest)
    - [16]          # Tiny: single layer, 16 neurons
    - [8, 4]        # Two tiny layers (risky)
  
  # Heavy regularization (critical for small data)
  alpha_range: [0.1, 1.0, 10.0]  # L2 penalty - use high values
  
  # Activation functions
  activation: ['relu']  # relu usually best, tanh sometimes good for finance
  
  # Solver - for small data
  solver: ['lbfgs']  # Best for <1000 samples, use 'adam' for larger
  
  # Learning rate (only for adam solver)
  learning_rate: ['adaptive']
  learning_rate_init: [0.001, 0.01]
  
  # Training parameters
  max_iter: 1000
  batch_size: 'auto'  # Let sklearn decide
  
  # Early stopping (critical!)
  early_stopping: true
  validation_fraction: 0.2
  n_iter_no_change: 20  # Stop if no improvement for 20 iterations
  
  # Other regularization
  momentum: 0.9
  beta_1: 0.9  # Adam parameter
  beta_2: 0.999  # Adam parameter
  epsilon: 0.00000001
  
  # Cross-validation
  cv_folds: 2  # Keep low for small data
  
  # Random state
  random_state: 42

# < 1000 samples
mlp_ultra_small:
  hidden_layer_sizes: [[8]]  # Single layer only
  alpha_range: [1.0, 10.0, 100.0]  # Extreme regularization
  solver: ['lbfgs']
  max_iter: 500
  cv_folds: 2

# 1000 - 5000 samples
mlp_small:
  hidden_layer_sizes: [[16], [32], [16, 8], [32, 16]]
  alpha_range: [0.01, 0.1, 1.0]
  activation: ['relu', 'tanh']
  solver: ['adam']
  learning_rate: ['adaptive', 'invscaling']
  learning_rate_init: [0.0001, 0.001, 0.01]
  max_iter: 1500
  early_stopping: true
  validation_fraction: 0.15
  n_iter_no_change: 30
  cv_folds: 3

# 5000 - 10,000 samples
mlp_medium:
  hidden_layer_sizes: [[50], [100], [50, 25], [100, 50], [100, 50, 25]]
  alpha_range: [0.001, 0.01, 0.1]
  activation: ['relu', 'tanh']
  solver: ['adam']
  learning_rate: ['constant', 'adaptive']
  learning_rate_init: [0.0001, 0.001]
  batch_size: [32, 64, 'auto']
  max_iter: 2000
  early_stopping: true
  validation_fraction: 0.15
  n_iter_no_change: 50
  cv_folds: 3

# > 10,000 samples
mlp_large:
  hidden_layer_sizes: [[100], [200], [100, 50], [200, 100], [200, 100, 50]]
  alpha_range: [0.0001, 0.001, 0.01]
  activation: ['relu', 'tanh']
  solver: ['adam']
  learning_rate: ['constant', 'invscaling', 'adaptive']
  learning_rate_init: [0.00001, 0.0001, 0.001]
  batch_size: [64, 128, 256]
  max_iter: 3000
  early_stopping: true
  validation_fraction: 0.1
  n_iter_no_change: 100
  cv_folds: 5